# -*- coding: utf-8 -*-
"""22i2063_BIGDATA_A3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uijxTdJqa1PJaQv8s4ebPKir3biokP3o

Pre-Processing
"""

import json

# Load the Sampled Amazon dataset
def load_dataset(file_path):
    with open('/content/meta_AMAZON_FASHION.json', 'r') as file:
        data = json.load(file)
    return data

# Preprocess the data
def preprocess_data(data):
    # Removing unnecessary fields
    for item in data:
        del item['imageURL']

    # Cleaning text data
    for item in data:
        item['title'] = item['title'].strip()

    # Converting price to numeric format
    for item in data:
        item['price'] = float(item['price']) if 'price' in item else None
    # Handling the  missing values
    for item in data:
        if 'price' not in item:
            item['price'] = 0.0  # Replacing missing price with 0.0

    return data


    cleaned_data = data
    return cleaned_data

# Generating a new JSON file containing the preprocessed data
def save_preprocessed_data(data, output_file):
    with open(output_file, 'w') as file:
        json.dump(data, file, indent=4)

# BONUS: Performing the  batch processing to execute pre-processing in real time
def batch_preprocessing(file_path, chunk_size, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(file_path, 'r') as file:
        chunk_num = 0
        while True:
            chunk = file.read(chunk_size)
            if not chunk:
                break
            chunk_data = json.loads(chunk)
            preprocessed_data = preprocess_data(chunk_data)
            output_file = os.path.join(output_dir, f'preprocessed_data_chunk_{chunk_num}.json')
            save_preprocessed_data(preprocessed_data, output_file)
            chunk_num += 1

"""Streaming Pipeline Setup"""

from kafka import KafkaProducer, KafkaConsumer

class DataProducer:
    def __init__(self, bootstrap_servers):
        self.producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

    def send_data(self, topic, data):
        self.producer.send(topic, json.dumps(data).encode('utf-8'))

class DataConsumer:
    def __init__(self, bootstrap_servers, topic):
        self.consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers)

    def consume_data(self):
        for message in self.consumer:
            data = json.loads(message.value.decode('utf-8'))

"""Frequent Itemset Mining

"""

def apriori_consumer():
    pass

def pcy_consumer():
    pass

def innovative_consumer():
    pass

"""Database Integration

"""

from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')
db = client['amazon_data']

def store_results(data, collection_name):
    collection = db[collection_name]
    collection.insert_many(data)